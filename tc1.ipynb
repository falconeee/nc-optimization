{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bc41771",
   "metadata": {},
   "source": [
    "# Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0425c956",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score, balanced_accuracy_score\n",
    "from scipy.optimize import differential_evolution\n",
    "import pygad\n",
    "from TfELM.Layers.ELMLayer import ELMLayer\n",
    "from TfELM.Models.ELMModel import ELMModel\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0568b98",
   "metadata": {},
   "source": [
    "# Wine Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00f884e",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6abf8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "\n",
    "data = load_wine()\n",
    "df_wine = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "df_wine['Wine'] = data.target\n",
    "\n",
    "print(df_wine.shape)\n",
    "print(\"Wine 1:\", df_wine[df_wine['Wine']==0].shape[0])\n",
    "print(\"Wine 2:\", df_wine[df_wine['Wine']==1].shape[0])\n",
    "print(\"Wine 3:\", df_wine[df_wine['Wine']==2].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f576a6",
   "metadata": {},
   "source": [
    "## Split train-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432c8681",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_wine.drop(columns=[\"Wine\"])\n",
    "y = df_wine[\"Wine\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y,random_state=42)\n",
    "\n",
    "print(\"X_train:\", X_train.shape)\n",
    "print(\"X_test:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8ab408",
   "metadata": {},
   "source": [
    "## Apply z-score in data train and data test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd92dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a01cd8",
   "metadata": {},
   "source": [
    "## Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8d2677",
   "metadata": {},
   "source": [
    "##### All features and default values for MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efae193",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLPClassifier(random_state=4).fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(\"*** Baseline MLPClassifier ***\")\n",
    "print(\"Accuracy:\", round(accuracy_score(y_test, y_pred),2))\n",
    "print(\"Precision:\", round(precision_score(y_test, y_pred, average='weighted'),2))\n",
    "print(\"F1 Score:\", round(f1_score(y_test, y_pred, average='weighted'),2))\n",
    "print(\"Recall:\", round(recall_score(y_test, y_pred, average='weighted'),2))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba932e92",
   "metadata": {},
   "source": [
    "## Differential Evolution (DE) + Backpropagation (BP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb00193c",
   "metadata": {},
   "source": [
    "Melhores configurações encontradas:\n",
    "- Features selecionadas: ['Alcohol', 'Ash', 'Acl', 'Phenols', 'Proanth', 'Color.int', 'Hue', 'OD', 'Proline']\n",
    "- Número de camadas ocultas: 2\n",
    "- Número de neurônios por camada: 31\n",
    "- Taxa de aprendizado: 0.03259\n",
    "- random_seed = 4\n",
    "- max_iter = 200\n",
    "\n",
    "Acurácia final no conjunto de teste: 1.0000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99af8076",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = X.columns.tolist()\n",
    "n_features = X_train_scaled.shape[1]\n",
    "\n",
    "def fitness(individual):\n",
    "    feature_mask = individual[:n_features] > 0.5\n",
    "    if not any(feature_mask):\n",
    "        return 10.0  # penaliza se nenhuma feature for selecionada\n",
    "\n",
    "    n_layers = int(np.clip(round(individual[n_features]), 1, 5))\n",
    "    n_neurons = int(np.clip(round(individual[n_features + 1]), 10, 100))\n",
    "    learning_rate = individual[n_features+2]\n",
    "\n",
    "    hidden_layer_sizes = tuple([n_neurons] * n_layers) # cria tupla com o número de camadas ocultas e com mesmo número de neurônios\n",
    "    X_sel = X_train_scaled[:, feature_mask] # seleciona as features com base no mask\n",
    "    # MLPClassifier com os parâmetros selecionados.\n",
    "    # Por padrão camada de entrada e saída são definidas automaticamente\n",
    "    # Camada de entrada tem o mesmo número de features selecionadas e a camada de saída tem o mesmo número de classes\n",
    "    clf = MLPClassifier(hidden_layer_sizes=hidden_layer_sizes,\n",
    "                        learning_rate_init=learning_rate,\n",
    "                        max_iter=200)\n",
    "\n",
    "    try:\n",
    "        score = cross_val_score(clf, X_sel, y_train, cv=3, scoring='accuracy').mean() # cross-validation com 3 folds\n",
    "    except:\n",
    "        return 10.0\n",
    "\n",
    "    return 1.0 - score  # minimizar o erro\n",
    "\n",
    "# DE com os parâmetros do problema\n",
    "bounds = [(0, 1)] * n_features + [(1, 5), (10, 100), (0.0001, 0.1)]\n",
    "\n",
    "result = differential_evolution(\n",
    "    fitness, bounds, maxiter=20, popsize=50, disp=True,\n",
    "    mutation=(0.5,1), recombination=0.95, strategy='best1bin'\n",
    ")\n",
    "\n",
    "# Avaliando resultado do melhor indivíduo encontrado\n",
    "best_individual = result.x\n",
    "selected_features = best_individual[:n_features] > 0.5\n",
    "n_layers_best = int(round(best_individual[n_features]))\n",
    "n_neurons_best = int(round(best_individual[n_features+1]))\n",
    "lr_best = best_individual[n_features+2]\n",
    "\n",
    "print(\"\\nMelhores configurações encontradas:\")\n",
    "print(f\"- Features selecionadas: {[feature_names[i] for i in range(n_features) if selected_features[i]]}\")\n",
    "print(f\"- Número de camadas ocultas: {n_layers_best}\")\n",
    "print(f\"- Número de neurônios por camada: {n_neurons_best}\")\n",
    "print(f\"- Taxa de aprendizado: {lr_best:.5f}\")\n",
    "\n",
    "# Treina modelo final com os melhores parâmetros\n",
    "clf_final = MLPClassifier(\n",
    "    hidden_layer_sizes=(n_neurons_best,) * n_layers_best,\n",
    "    learning_rate_init=lr_best,\n",
    "    max_iter=200,\n",
    "    random_state=4\n",
    ")\n",
    "\n",
    "X_train_sel = X_train_scaled[:, selected_features]\n",
    "X_test_sel = X_test_scaled[:, selected_features]\n",
    "\n",
    "clf_final.fit(X_train_sel, y_train)\n",
    "accuracy_test = clf_final.score(X_test_sel, y_test)\n",
    "print(f\"\\nAcurácia final no conjunto de teste: {accuracy_test:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01d8330",
   "metadata": {},
   "source": [
    "## Differential Evolution (DE) + Extreme Learning Machine (ELM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2900382",
   "metadata": {},
   "source": [
    "Melhores configurações encontradas:\n",
    "- Features selecionadas: ['Alcohol', 'Ash', 'Flavanoids', 'Nonflavanoid.phenols', 'Color.int', 'Hue', 'Proline']\n",
    "- Número de neurônios ocultos (ELM): 293\n",
    "\n",
    "Acurácia final no conjunto de teste com ELM: 0.9778"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ed6115",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = X_train_scaled.shape[1]\n",
    "\n",
    "# A função fitness agora otimiza apenas a seleção de features e o número de neurônios\n",
    "def fitness(individual):\n",
    "    feature_mask = individual[:n_features] > 0.5\n",
    "    if not np.any(feature_mask):\n",
    "        return 10.0\n",
    "\n",
    "    # O único hiperparâmetro do ELM a ser otimizado é o número de neurônios.\n",
    "    n_neurons = int(round(individual[n_features]))\n",
    "    \n",
    "    X_sel = X_train_scaled[:, feature_mask]\n",
    "    \n",
    "    # MLPClassifier -> ELMClassifier\n",
    "    # 'number_neurons' é o parâmetro para o número de neurônios da camada oculta\n",
    "    elm = ELMLayer(number_neurons=n_neurons, activation='relu')\n",
    "    clf = ELMModel(elm)\n",
    "\n",
    "    try:\n",
    "        score = cross_val_score(clf, X_sel, y_train, cv=3, scoring='accuracy').mean()\n",
    "    except Exception as e:\n",
    "        # Penaliza se houver qualquer erro durante o treinamento/validação\n",
    "        return 10.0\n",
    "\n",
    "    return 1.0 - score  # O objetivo continua sendo minimizar o erro (1 - acurácia)\n",
    "\n",
    "# bounds apenas para seleção de features e número de neurônios\n",
    "bounds = [(0, 1)] * n_features + [(10, 1000)]\n",
    "\n",
    "result = differential_evolution(\n",
    "    fitness, bounds, maxiter=20, popsize=50, disp=True,\n",
    "    mutation=(0.7, 1.5), recombination=0.7, strategy='best1bin'\n",
    ")\n",
    "\n",
    "best_individual = result.x\n",
    "selected_features = best_individual[:n_features] > 0.5\n",
    "n_neurons_best = int(round(best_individual[n_features]))\n",
    "\n",
    "print(\"\\nMelhores configurações encontradas:\")\n",
    "print(f\"- Features selecionadas: {[feature_names[i] for i in range(n_features) if selected_features[i]]}\")\n",
    "print(f\"- Número de neurônios ocultos (ELM): {n_neurons_best}\")\n",
    "\n",
    "elm = ELMLayer(number_neurons=n_neurons_best, activation='relu')\n",
    "clf_final = ELMModel(elm)\n",
    "\n",
    "X_train_sel = X_train_scaled[:, selected_features]\n",
    "X_test_sel = X_test_scaled[:, selected_features]\n",
    "\n",
    "clf_final.fit(X_train_sel, y_train)\n",
    "accuracy_test = clf_final.score(X_test_sel, y_test)\n",
    "print(f\"\\nAcurácia final no conjunto de teste com ELM: {accuracy_test:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212c6efd",
   "metadata": {},
   "source": [
    "## Genetic Algorithm (GA) + Backpropagation (BP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a096ecea",
   "metadata": {},
   "source": [
    "Melhores configurações encontradas:\n",
    "- Fitness (Acurácia em CV): 1.0000\n",
    "- Features selecionadas: ['Alcohol', 'Malic.acid', 'Ash', 'Acl', 'Mg', 'Flavanoids', 'Nonflavanoid.phenols', 'Proanth', 'Color.int', 'Hue', 'Proline']\n",
    "- Número de camadas ocultas: 1\n",
    "- Número de neurônios por camada: 18\n",
    "- Taxa de aprendizado: 0.07042\n",
    "\n",
    "Acurácia final no conjunto de teste: 0.9778"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da08e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = X_train_scaled.shape[1]\n",
    "feature_names = X.columns.tolist()\n",
    "\n",
    "def fitness_func(ga_instance, solution, solution_idx):\n",
    "    feature_mask = np.array(solution[:n_features]).astype(bool)\n",
    "    \n",
    "    # Penaliza se nenhuma característica for selecionada\n",
    "    if not np.any(feature_mask):\n",
    "        return -1.0  # PyGAD maximiza, então um valor baixo para penalizar\n",
    "\n",
    "    # Arredonda os valores para garantir que sejam inteiros\n",
    "    n_layers = int(round(solution[n_features]))\n",
    "    n_neurons = int(round(solution[n_features + 1]))\n",
    "    learning_rate = solution[n_features + 2]\n",
    "\n",
    "    hidden_layer_sizes = tuple([n_neurons] * n_layers)\n",
    "    X_sel = X_train_scaled[:, feature_mask]\n",
    "\n",
    "    clf = MLPClassifier(hidden_layer_sizes=hidden_layer_sizes,\n",
    "                        learning_rate_init=learning_rate,\n",
    "                        max_iter=200\n",
    "                        )\n",
    "\n",
    "    try:\n",
    "        # cross-validation com 3 folds\n",
    "        score = cross_val_score(clf, X_sel, y_train, cv=3, scoring='accuracy').mean()\n",
    "    except ValueError:\n",
    "        # penaliza a solução em caso de erro\n",
    "        return -1.0\n",
    "\n",
    "    # PyGAD maximiza a função de fitness por padrão, então a acurácia é retornada diretamente.\n",
    "    return score\n",
    "\n",
    "# Define o espaço de busca\n",
    "gene_space = [ [0, 1] for _ in range(n_features) ] + [ {'low': 1, 'high': 3, 'step': 1}, {'low': 10, 'high': 100, 'step': 1}, {'low': 0.0001, 'high': 0.1} ]\n",
    "\n",
    "num_genes = len(gene_space)\n",
    "ga_instance = pygad.GA(\n",
    "    num_generations=50,\n",
    "    num_parents_mating=10,\n",
    "    sol_per_pop=50,\n",
    "    num_genes=num_genes,\n",
    "    fitness_func=fitness_func,\n",
    "    gene_space=gene_space,\n",
    "    parent_selection_type=\"tournament\",\n",
    "    crossover_type=\"two_points\",\n",
    "    mutation_type=\"random\",\n",
    "    mutation_probability=0.6,\n",
    ")\n",
    "ga_instance.run()\n",
    "ga_instance.plot_fitness()\n",
    "\n",
    "best_solution, best_solution_fitness, best_solution_idx = ga_instance.best_solution()\n",
    "\n",
    "selected_features = best_solution[:n_features].astype(bool)\n",
    "n_layers_best = int(round(best_solution[n_features]))\n",
    "n_neurons_best = int(round(best_solution[n_features+1]))\n",
    "lr_best = best_solution[n_features+2]\n",
    "\n",
    "print(\"\\nMelhores configurações encontradas:\")\n",
    "print(f\"- Fitness (Acurácia em CV): {best_solution_fitness:.4f}\")\n",
    "print(f\"- Features selecionadas: {[feature_names[i] for i in range(n_features) if selected_features[i]]}\")\n",
    "print(f\"- Número de camadas ocultas: {n_layers_best}\")\n",
    "print(f\"- Número de neurônios por camada: {n_neurons_best}\")\n",
    "print(f\"- Taxa de aprendizado: {lr_best:.5f}\")\n",
    "\n",
    "# Treina o modelo final com os melhores parâmetros\n",
    "clf_final = MLPClassifier(\n",
    "    hidden_layer_sizes=(n_neurons_best,) * n_layers_best,\n",
    "    learning_rate_init=lr_best,\n",
    "    max_iter=200,\n",
    "    random_state=4\n",
    ")\n",
    "\n",
    "X_train_sel = X_train_scaled[:, selected_features]\n",
    "X_test_sel = X_test_scaled[:, selected_features]\n",
    "\n",
    "clf_final.fit(X_train_sel, y_train)\n",
    "accuracy_test = clf_final.score(X_test_sel, y_test)\n",
    "print(f\"\\nAcurácia final no conjunto de teste: {accuracy_test:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b88cc4d",
   "metadata": {},
   "source": [
    "## Genetic Algorithm (GA) + Extreme Learning Machine (ELM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026f32a1",
   "metadata": {},
   "source": [
    "Melhores configurações encontradas:\n",
    "- Fitness (Acurácia em CV): 1.0000\n",
    "- Features selecionadas: ['Alcohol', 'Ash', 'Acl', 'Flavanoids', 'Hue', 'OD', 'Proline']\n",
    "- Número de neurônios da camada oculta: 48\n",
    "\n",
    "Acurácia final no conjunto de teste: 0.9556"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219b654f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = X_train_scaled.shape[1]\n",
    "feature_names = X.columns.tolist()\n",
    "\n",
    "def fitness_func(ga_instance, solution, solution_idx):\n",
    "    feature_mask = np.array(solution[:n_features]).astype(bool)\n",
    "    \n",
    "    # Penaliza se nenhuma característica for selecionada\n",
    "    if not np.any(feature_mask):\n",
    "        return -1.0  # PyGAD maximiza, então um valor baixo para penalizar\n",
    "\n",
    "    # Arredonda os valores para garantir que sejam inteiros\n",
    "    n_neurons = int(round(solution[n_features + 1]))\n",
    "\n",
    "    X_sel = X_train_scaled[:, feature_mask]\n",
    "\n",
    "    elm = ELMLayer(number_neurons=n_neurons, activation='relu')\n",
    "    clf = ELMModel(elm)\n",
    "\n",
    "    try:\n",
    "        # cross-validation com 3 folds\n",
    "        score = cross_val_score(clf, X_sel, y_train, cv=3, scoring='accuracy').mean()\n",
    "    except ValueError:\n",
    "        # penaliza a solução em caso de erro\n",
    "        return -1.0\n",
    "\n",
    "    # PyGAD maximiza a função de fitness por padrão, então a acurácia é retornada diretamente.\n",
    "    return score\n",
    "\n",
    "for _ in range(n_features):\n",
    "    gene_space.append({'low': 0, 'high': 1, 'step': 1})\n",
    "gene_space.append({'low': 10, 'high': 1000, 'step': 1})\n",
    "\n",
    "num_genes = len(gene_space)\n",
    "ga_instance = pygad.GA(\n",
    "    num_generations=50,\n",
    "    num_parents_mating=10,\n",
    "    sol_per_pop=50,\n",
    "    num_genes=num_genes,              \n",
    "    fitness_func=fitness_func,\n",
    "    gene_space=gene_space,\n",
    "    parent_selection_type=\"tournament\",\n",
    "    crossover_type=\"two_points\",\n",
    "    mutation_type=\"random\",\n",
    "    mutation_probability=0.6\n",
    ")\n",
    "ga_instance.run()\n",
    "ga_instance.plot_fitness()\n",
    "\n",
    "best_solution, best_solution_fitness, best_solution_idx = ga_instance.best_solution()\n",
    "\n",
    "selected_features = best_solution[:n_features].astype(bool)\n",
    "n_neurons_best = int(round(best_solution[n_features + 1]))\n",
    "\n",
    "print(\"\\nMelhores configurações encontradas:\")\n",
    "print(f\"- Fitness (Acurácia em CV): {best_solution_fitness:.4f}\")\n",
    "print(f\"- Features selecionadas: {[feature_names[i] for i in range(n_features) if selected_features[i]]}\")\n",
    "print(f\"- Número de neurônios da camada oculta: {n_neurons_best}\")\n",
    "\n",
    "# Treina o modelo final com os melhores parâmetros\n",
    "elm = ELMLayer(number_neurons=n_neurons_best, activation='relu')\n",
    "clf_final = ELMModel(elm)\n",
    "\n",
    "X_train_sel = X_train_scaled[:, selected_features]\n",
    "X_test_sel = X_test_scaled[:, selected_features]\n",
    "\n",
    "clf_final.fit(X_train_sel, y_train)\n",
    "accuracy_test = clf_final.score(X_test_sel, y_test)\n",
    "print(f\"\\nAcurácia final no conjunto de teste: {accuracy_test:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe2ba6c",
   "metadata": {},
   "source": [
    "# Diabetes dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2681f303",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9ef48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_diabetes = pd.read_csv(\"diabetes.csv\", sep=\",\", decimal=\".\")\n",
    "print(df_diabetes.shape)\n",
    "\n",
    "print(\"Class 0:\", df_diabetes[df_diabetes['Outcome']==0].shape[0])\n",
    "print(\"Class 1:\", df_diabetes[df_diabetes['Outcome']==1].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72e3257",
   "metadata": {},
   "source": [
    "## Split train-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132ef0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_diabetes.drop(columns=[\"Outcome\"])\n",
    "y = df_diabetes[\"Outcome\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y,random_state=42)\n",
    "\n",
    "print(\"X_train:\", X_train.shape)\n",
    "print(\"X_test:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a507dd2",
   "metadata": {},
   "source": [
    "## Apply z-score in data train and data test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70f05a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e5aa49",
   "metadata": {},
   "source": [
    "## Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6112c133",
   "metadata": {},
   "source": [
    "##### All features and default values for MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c713609",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLPClassifier(random_state=4).fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(\"*** Baseline MLPClassifier ***\")\n",
    "print(\"Accuracy:\", round(accuracy_score(y_test, y_pred),2))\n",
    "print(\"Precision:\", round(precision_score(y_test, y_pred, average='weighted'),2))\n",
    "print(\"F1 Score:\", round(f1_score(y_test, y_pred, average='weighted'),2))\n",
    "print(\"Recall:\", round(recall_score(y_test, y_pred, average='weighted'),2))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7613c9a2",
   "metadata": {},
   "source": [
    "## Differential Evolution (DE) + Backpropagation (BP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7deb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = X.columns.tolist()\n",
    "n_features = X_train_scaled.shape[1]\n",
    "\n",
    "def fitness(individual):\n",
    "    feature_mask = individual[:n_features] > 0.5\n",
    "    if not any(feature_mask):\n",
    "        return 10.0  # penaliza se nenhuma feature for selecionada\n",
    "\n",
    "    n_layers = int(np.clip(round(individual[n_features]), 1, 5))\n",
    "    n_neurons = int(np.clip(round(individual[n_features + 1]), 10, 100))\n",
    "    learning_rate = individual[n_features+2]\n",
    "\n",
    "    hidden_layer_sizes = tuple([n_neurons] * n_layers) # cria tupla com o número de camadas ocultas e com mesmo número de neurônios\n",
    "    X_sel = X_train_scaled[:, feature_mask] # seleciona as features com base no mask\n",
    "    # MLPClassifier com os parâmetros selecionados.\n",
    "    # Por padrão camada de entrada e saída são definidas automaticamente\n",
    "    # Camada de entrada tem o mesmo número de features selecionadas e a camada de saída tem o mesmo número de classes\n",
    "    clf = MLPClassifier(hidden_layer_sizes=hidden_layer_sizes,\n",
    "                        learning_rate_init=learning_rate,\n",
    "                        max_iter=200)\n",
    "\n",
    "    try:\n",
    "        score = cross_val_score(clf, X_sel, y_train, cv=3, scoring='accuracy').mean() # cross-validation com 3 folds\n",
    "    except:\n",
    "        return 10.0\n",
    "\n",
    "    return 1.0 - score  # minimizar o erro\n",
    "\n",
    "# DE com os parâmetros do problema\n",
    "bounds = [(0, 1)] * n_features + [(1, 5), (10, 100), (0.0001, 0.1)]\n",
    "\n",
    "result = differential_evolution(\n",
    "    fitness, bounds, maxiter=20, popsize=50, disp=True,\n",
    "    mutation=(0.5,1), recombination=0.95, strategy='best1bin'\n",
    ")\n",
    "\n",
    "# Avaliando resultado do melhor indivíduo encontrado\n",
    "best_individual = result.x\n",
    "selected_features = best_individual[:n_features] > 0.5\n",
    "n_layers_best = int(round(best_individual[n_features]))\n",
    "n_neurons_best = int(round(best_individual[n_features+1]))\n",
    "lr_best = best_individual[n_features+2]\n",
    "\n",
    "print(\"\\nMelhores configurações encontradas:\")\n",
    "print(f\"- Features selecionadas: {[feature_names[i] for i in range(n_features) if selected_features[i]]}\")\n",
    "print(f\"- Número de camadas ocultas: {n_layers_best}\")\n",
    "print(f\"- Número de neurônios por camada: {n_neurons_best}\")\n",
    "print(f\"- Taxa de aprendizado: {lr_best:.5f}\")\n",
    "\n",
    "# Treina modelo final com os melhores parâmetros\n",
    "clf_final = MLPClassifier(\n",
    "    hidden_layer_sizes=(n_neurons_best,) * n_layers_best,\n",
    "    learning_rate_init=lr_best,\n",
    "    max_iter=200,\n",
    "    random_state=4\n",
    ")\n",
    "\n",
    "X_train_sel = X_train_scaled[:, selected_features]\n",
    "X_test_sel = X_test_scaled[:, selected_features]\n",
    "\n",
    "clf_final.fit(X_train_sel, y_train)\n",
    "accuracy_test = clf_final.score(X_test_sel, y_test)\n",
    "print(f\"\\nAcurácia final no conjunto de teste: {accuracy_test:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0dd4f53",
   "metadata": {},
   "source": [
    "## Differential Evolution (DE) + Extreme Learning Machine (ELM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719692a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = X_train_scaled.shape[1]\n",
    "\n",
    "# A função fitness agora otimiza apenas a seleção de features e o número de neurônios\n",
    "def fitness(individual):\n",
    "    feature_mask = individual[:n_features] > 0.5\n",
    "    if not np.any(feature_mask):\n",
    "        return 10.0\n",
    "\n",
    "    # O único hiperparâmetro do ELM a ser otimizado é o número de neurônios.\n",
    "    n_neurons = int(round(individual[n_features]))\n",
    "    \n",
    "    X_sel = X_train_scaled[:, feature_mask]\n",
    "    \n",
    "    # MLPClassifier -> ELMClassifier\n",
    "    # 'number_neurons' é o parâmetro para o número de neurônios da camada oculta\n",
    "    elm = ELMLayer(number_neurons=n_neurons, activation='relu')\n",
    "    clf = ELMModel(elm)\n",
    "\n",
    "    try:\n",
    "        score = cross_val_score(clf, X_sel, y_train, cv=3, scoring='accuracy').mean()\n",
    "    except Exception as e:\n",
    "        # Penaliza se houver qualquer erro durante o treinamento/validação\n",
    "        return 10.0\n",
    "\n",
    "    return 1.0 - score  # O objetivo continua sendo minimizar o erro (1 - acurácia)\n",
    "\n",
    "# bounds apenas para seleção de features e número de neurônios\n",
    "bounds = [(0, 1)] * n_features + [(10, 1000)]\n",
    "\n",
    "result = differential_evolution(\n",
    "    fitness, bounds, maxiter=20, popsize=50, disp=True,\n",
    "    mutation=(0.7, 1.5), recombination=0.7, strategy='best1bin'\n",
    ")\n",
    "\n",
    "best_individual = result.x\n",
    "selected_features = best_individual[:n_features] > 0.5\n",
    "n_neurons_best = int(round(best_individual[n_features]))\n",
    "\n",
    "print(\"\\nMelhores configurações encontradas:\")\n",
    "print(f\"- Features selecionadas: {[feature_names[i] for i in range(n_features) if selected_features[i]]}\")\n",
    "print(f\"- Número de neurônios ocultos (ELM): {n_neurons_best}\")\n",
    "\n",
    "elm = ELMLayer(number_neurons=n_neurons_best, activation='relu')\n",
    "clf_final = ELMModel(elm)\n",
    "\n",
    "X_train_sel = X_train_scaled[:, selected_features]\n",
    "X_test_sel = X_test_scaled[:, selected_features]\n",
    "\n",
    "clf_final.fit(X_train_sel, y_train)\n",
    "accuracy_test = clf_final.score(X_test_sel, y_test)\n",
    "print(f\"\\nAcurácia final no conjunto de teste com ELM: {accuracy_test:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8932e0",
   "metadata": {},
   "source": [
    "## Genetic Algorithm (GA) + Backpropagation (BP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd6f0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = X_train_scaled.shape[1]\n",
    "feature_names = X.columns.tolist()\n",
    "\n",
    "def fitness_func(ga_instance, solution, solution_idx):\n",
    "    feature_mask = np.array(solution[:n_features]).astype(bool)\n",
    "    \n",
    "    # Penaliza se nenhuma característica for selecionada\n",
    "    if not np.any(feature_mask):\n",
    "        return -1.0  # PyGAD maximiza, então um valor baixo para penalizar\n",
    "\n",
    "    # Arredonda os valores para garantir que sejam inteiros\n",
    "    n_layers = int(round(solution[n_features]))\n",
    "    n_neurons = int(round(solution[n_features + 1]))\n",
    "    learning_rate = solution[n_features + 2]\n",
    "\n",
    "    hidden_layer_sizes = tuple([n_neurons] * n_layers)\n",
    "    X_sel = X_train_scaled[:, feature_mask]\n",
    "\n",
    "    clf = MLPClassifier(hidden_layer_sizes=hidden_layer_sizes,\n",
    "                        learning_rate_init=learning_rate,\n",
    "                        max_iter=200\n",
    "                        )\n",
    "\n",
    "    try:\n",
    "        # cross-validation com 3 folds\n",
    "        score = cross_val_score(clf, X_sel, y_train, cv=3, scoring='accuracy').mean()\n",
    "    except ValueError:\n",
    "        # penaliza a solução em caso de erro\n",
    "        return -1.0\n",
    "\n",
    "    # PyGAD maximiza a função de fitness por padrão, então a acurácia é retornada diretamente.\n",
    "    return score\n",
    "\n",
    "# Define o espaço de busca\n",
    "gene_space = [ [0, 1] for _ in range(n_features) ] + [ {'low': 1, 'high': 3, 'step': 1}, {'low': 10, 'high': 100, 'step': 1}, {'low': 0.0001, 'high': 0.1} ]\n",
    "\n",
    "num_genes = len(gene_space)\n",
    "ga_instance = pygad.GA(\n",
    "    num_generations=50,\n",
    "    num_parents_mating=10,\n",
    "    sol_per_pop=50,\n",
    "    num_genes=num_genes,\n",
    "    fitness_func=fitness_func,\n",
    "    gene_space=gene_space,\n",
    "    parent_selection_type=\"tournament\",\n",
    "    crossover_type=\"two_points\",\n",
    "    mutation_type=\"random\",\n",
    "    mutation_probability=0.6,\n",
    ")\n",
    "ga_instance.run()\n",
    "ga_instance.plot_fitness()\n",
    "\n",
    "best_solution, best_solution_fitness, best_solution_idx = ga_instance.best_solution()\n",
    "\n",
    "selected_features = best_solution[:n_features].astype(bool)\n",
    "n_layers_best = int(round(best_solution[n_features]))\n",
    "n_neurons_best = int(round(best_solution[n_features+1]))\n",
    "lr_best = best_solution[n_features+2]\n",
    "\n",
    "print(\"\\nMelhores configurações encontradas:\")\n",
    "print(f\"- Fitness (Acurácia em CV): {best_solution_fitness:.4f}\")\n",
    "print(f\"- Features selecionadas: {[feature_names[i] for i in range(n_features) if selected_features[i]]}\")\n",
    "print(f\"- Número de camadas ocultas: {n_layers_best}\")\n",
    "print(f\"- Número de neurônios por camada: {n_neurons_best}\")\n",
    "print(f\"- Taxa de aprendizado: {lr_best:.5f}\")\n",
    "\n",
    "# Treina o modelo final com os melhores parâmetros\n",
    "clf_final = MLPClassifier(\n",
    "    hidden_layer_sizes=(n_neurons_best,) * n_layers_best,\n",
    "    learning_rate_init=lr_best,\n",
    "    max_iter=200,\n",
    "    random_state=4\n",
    ")\n",
    "\n",
    "X_train_sel = X_train_scaled[:, selected_features]\n",
    "X_test_sel = X_test_scaled[:, selected_features]\n",
    "\n",
    "clf_final.fit(X_train_sel, y_train)\n",
    "accuracy_test = clf_final.score(X_test_sel, y_test)\n",
    "print(f\"\\nAcurácia final no conjunto de teste: {accuracy_test:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1b33f3",
   "metadata": {},
   "source": [
    "## Genetic Algorithm (GA) + Extreme Learning Machine (ELM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166923f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = X_train_scaled.shape[1]\n",
    "feature_names = X.columns.tolist()\n",
    "\n",
    "def fitness_func(ga_instance, solution, solution_idx):\n",
    "    feature_mask = np.array(solution[:n_features]).astype(bool)\n",
    "    \n",
    "    # Penaliza se nenhuma característica for selecionada\n",
    "    if not np.any(feature_mask):\n",
    "        return -1.0  # PyGAD maximiza, então um valor baixo para penalizar\n",
    "\n",
    "    # Arredonda os valores para garantir que sejam inteiros\n",
    "    n_neurons = int(round(solution[n_features + 1]))\n",
    "\n",
    "    X_sel = X_train_scaled[:, feature_mask]\n",
    "\n",
    "    elm = ELMLayer(number_neurons=n_neurons, activation='relu')\n",
    "    clf = ELMModel(elm)\n",
    "\n",
    "    try:\n",
    "        # cross-validation com 3 folds\n",
    "        score = cross_val_score(clf, X_sel, y_train, cv=3, scoring='accuracy').mean()\n",
    "    except ValueError:\n",
    "        # penaliza a solução em caso de erro\n",
    "        return -1.0\n",
    "\n",
    "    # PyGAD maximiza a função de fitness por padrão, então a acurácia é retornada diretamente.\n",
    "    return score\n",
    "\n",
    "for _ in range(n_features):\n",
    "    gene_space.append({'low': 0, 'high': 1, 'step': 1})\n",
    "gene_space.append({'low': 10, 'high': 1000, 'step': 1})\n",
    "\n",
    "num_genes = len(gene_space)\n",
    "ga_instance = pygad.GA(\n",
    "    num_generations=50,\n",
    "    num_parents_mating=10,\n",
    "    sol_per_pop=50,\n",
    "    num_genes=num_genes,              \n",
    "    fitness_func=fitness_func,\n",
    "    gene_space=gene_space,\n",
    "    parent_selection_type=\"tournament\",\n",
    "    crossover_type=\"two_points\",\n",
    "    mutation_type=\"random\",\n",
    "    mutation_probability=0.6\n",
    ")\n",
    "ga_instance.run()\n",
    "ga_instance.plot_fitness()\n",
    "\n",
    "best_solution, best_solution_fitness, best_solution_idx = ga_instance.best_solution()\n",
    "\n",
    "selected_features = best_solution[:n_features].astype(bool)\n",
    "n_neurons_best = int(round(best_solution[n_features + 1]))\n",
    "\n",
    "print(\"\\nMelhores configurações encontradas:\")\n",
    "print(f\"- Fitness (Acurácia em CV): {best_solution_fitness:.4f}\")\n",
    "print(f\"- Features selecionadas: {[feature_names[i] for i in range(n_features) if selected_features[i]]}\")\n",
    "print(f\"- Número de neurônios da camada oculta: {n_neurons_best}\")\n",
    "\n",
    "# Treina o modelo final com os melhores parâmetros\n",
    "elm = ELMLayer(number_neurons=n_neurons_best, activation='relu')\n",
    "clf_final = ELMModel(elm)\n",
    "\n",
    "X_train_sel = X_train_scaled[:, selected_features]\n",
    "X_test_sel = X_test_scaled[:, selected_features]\n",
    "\n",
    "clf_final.fit(X_train_sel, y_train)\n",
    "accuracy_test = clf_final.score(X_test_sel, y_test)\n",
    "print(f\"\\nAcurácia final no conjunto de teste: {accuracy_test:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f6b706",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
